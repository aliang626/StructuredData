import pandas as pd
import psycopg2
from sqlalchemy import create_engine, text, inspect
from app.models.data_source import DataSource, TableField
from app import db
import configparser
import os
import pandas as pd
from sqlalchemy import create_engine
import re
from pathlib import Path

def read_sql_auto_encoding(query, engine):
    """è‡ªåŠ¨å¤„ç†ç¼–ç çš„SQLè¯»å–å‡½æ•°"""
    try:
        print("å°è¯•ä½¿ç”¨é»˜è®¤ç¼–ç è¯»å–æ•°æ®...")
        df = pd.read_sql(query, engine)
        print("æˆåŠŸä½¿ç”¨é»˜è®¤ç¼–ç è¯»å–æ•°æ®")
        return df
    except UnicodeDecodeError as e:
        print(f"é»˜è®¤ç¼–ç è§£ç å¤±è´¥: {str(e)}")
        # å¦‚æœé»˜è®¤ç¼–ç å¤±è´¥ï¼Œå°è¯•é‡æ–°åˆ›å»ºå¼•æ“å¹¶ä½¿ç”¨ä¸åŒç¼–ç 
        return None
    except Exception as e:
        print(f"è¯»å–æ•°æ®å¤±è´¥: {str(e)}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
        return None


def read_csv_auto_encoding(file_path):
    """è‡ªåŠ¨æ£€æµ‹CSVæ–‡ä»¶ç¼–ç å¹¶è¯»å–"""
    encodings = ['utf-8', 'gbk', 'latin-1']
    
    for encoding in encodings:
        try:
            return pd.read_csv(file_path, encoding=encoding)
        except UnicodeDecodeError:
            continue
    
    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨é”™è¯¯å¤„ç†æ¨¡å¼
    try:
        return pd.read_csv(file_path, encoding='utf-8', errors='ignore')
    except Exception as e:
        raise Exception(f"æ— æ³•è¯»å–CSVæ–‡ä»¶ {file_path}: {str(e)}")

class DatabaseService:
    """æ•°æ®åº“æœåŠ¡ç±»"""
    
    @staticmethod
    def quote_identifier(identifier):
        """ä¸ºPostgreSQLæ ‡è¯†ç¬¦æ·»åŠ åŒå¼•å·ï¼Œå¤„ç†å¤§å°å†™æ•æ„Ÿé—®é¢˜"""
        if identifier is None:
            return None
        # å¦‚æœæ ‡è¯†ç¬¦åŒ…å«å¤§å†™å­—æ¯ã€ç‰¹æ®Šå­—ç¬¦æˆ–ç©ºæ ¼ï¼Œåˆ™æ·»åŠ å¼•å·
        if (any(c.isupper() for c in identifier) or 
            any(c in identifier for c in [' ', '(', ')', 'Â°', '-', '.']) or
            identifier != identifier.lower()):
            return f'"{identifier}"'
        return identifier
    
    @staticmethod
    def get_connection_string(db_config, encoding='utf8'):
        """è·å–æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²ï¼Œæ”¯æŒæŒ‡å®šç¼–ç """
        # åŸºç¡€ URL
        base_url = f"postgresql://{db_config['username']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}"
        
        # [æ ¸å¿ƒä¿®å¤] å°† client_encoding æ˜¾å¼æ‹¼æ¥åˆ° URL ä¸­
        if encoding:
            pg_encoding = encoding
            # æ˜ å°„å¸¸ç”¨ç¼–ç åç§°åˆ° PostgreSQL æ”¯æŒçš„æ ‡å‡†åç§°
            if encoding.lower() in ['utf-8', 'utf8']:
                pg_encoding = 'UTF8'
            elif encoding.lower() in ['gbk', 'gb18030']:
                pg_encoding = 'GBK'
            elif encoding.lower() in ['latin1', 'latin-1']:
                pg_encoding = 'LATIN1'
            
            # æ‹¼æ¥å‚æ•°ï¼Œç¡®ä¿é©±åŠ¨å±‚è¯†åˆ«
            return f"{base_url}?client_encoding={pg_encoding}"
            
        return base_url

    @staticmethod
    def create_engine(connection_string, **kwargs):
        """åˆ›å»ºæ•°æ®åº“å¼•æ“ - ä½¿ç”¨NullPoolé¿å…è¿æ¥æ± å¤ç”¨é—®é¢˜"""
        from sqlalchemy.pool import NullPool
        default_args = {
            'poolclass': NullPool,  # å…³é”®ä¿®å¤ï¼šä½¿ç”¨NullPoolï¼Œæ¯æ¬¡åˆ›å»ºæ–°è¿æ¥ï¼Œç”¨å®Œç«‹å³å…³é—­
            'pool_pre_ping': True,
            'echo': False
        }
        default_args.update(kwargs)
        return create_engine(connection_string, **default_args)
    
    @staticmethod
    def test_connection(db_config):
        """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
        try:
            conn = psycopg2.connect(
                host=db_config['host'],
                port=int(db_config['port']),
                user=db_config['username'],
                password=db_config['password'],
                database=db_config['database'],
                client_encoding='utf8'
            )
            conn.close()
            return True
        except Exception as e:
            print(f"æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")
            return False
    
    @staticmethod
    def get_schemas(db_config):
        """è·å–æ•°æ®åº“æ‰€æœ‰schemaåˆ—è¡¨"""
        try:
            encodings = ['utf8', 'gbk', 'latin1']
            last_error = None
            
            for enc in encodings:
                try:
                    connection_string = DatabaseService.get_connection_string(db_config, enc)
                    engine = create_engine(
                        connection_string, 
                        connect_args={'client_encoding': enc},
                        pool_pre_ping=True,
                        echo=False
                    )
                    
                    with engine.connect() as conn:
                        try:
                            conn.execute(text(f"SET client_encoding = '{enc}'"))
                        except Exception as enc_error:
                            print(f"è®¾ç½®ç¼–ç  {enc} å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç : {str(enc_error)}")
                        
                        # æŸ¥è¯¢æ‰€æœ‰schema
                        query = """
                        SELECT schema_name
                        FROM information_schema.schemata
                        WHERE schema_name NOT IN ('pg_catalog', 'information_schema', 'pg_toast')
                        ORDER BY schema_name
                        """
                        
                        result = conn.execute(text(query))
                        schemas = [row[0] for row in result.fetchall()]
                        
                        print(f"æˆåŠŸä½¿ç”¨ç¼–ç  {enc} è·å–åˆ° {len(schemas)} ä¸ªschema")
                        return schemas
                        
                except Exception as e:
                    print(f"ç¼–ç  {enc} å¤±è´¥: {str(e)}")
                    last_error = e
                    continue
            
            # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤çš„public schema
            print(f"æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥ï¼Œè¿”å›é»˜è®¤schemaã€‚æœ€åé”™è¯¯: {last_error}")
            return ['public']
            
        except Exception as e:
            print(f"è·å–schemaåˆ—è¡¨å¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤schemaè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return ['public']
    
    @staticmethod
    def get_tables(db_config):
        """è·å–æ•°æ®åº“è¡¨åˆ—è¡¨ï¼ˆåŒ…å«æè¿°ï¼‰ï¼Œè‡ªåŠ¨å°è¯•å¤šç§ç¼–ç """
        try:
            encodings = ['utf8', 'gbk', 'latin1']
            last_error = None
            
            # è·å–schemaï¼Œé»˜è®¤ä¸ºpublic
            schema = db_config.get('schema', 'public')
            
            for enc in encodings:
                try:
                    connection_string = DatabaseService.get_connection_string(db_config, enc)
                    engine = create_engine(
                        connection_string, 
                        connect_args={'client_encoding': enc},
                        pool_pre_ping=True,
                        echo=False
                    )
                    
                    with engine.connect() as conn:
                        try:
                            conn.execute(text(f"SET client_encoding = '{enc}'"))
                        except Exception as enc_error:
                            print(f"è®¾ç½®ç¼–ç  {enc} å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç : {str(enc_error)}")
                        
                        # æŸ¥è¯¢è¡¨åå’Œæè¿°ï¼Œä½¿ç”¨åŠ¨æ€schema
                        query = """
                        SELECT 
                            c.relname as table_name,
                            COALESCE(d.description, '') as table_description
                        FROM pg_class c
                        LEFT JOIN pg_description d ON c.oid = d.objoid AND d.objsubid = 0
                        WHERE c.relkind = 'r' 
                        AND c.relnamespace = (SELECT oid FROM pg_namespace WHERE nspname = :schema)
                        ORDER BY c.relname
                        """
                        
                        result = conn.execute(text(query), {'schema': schema})
                        tables = []
                        for row in result.fetchall():
                            tables.append({
                                'name': row[0],
                                'description': row[1] if row[1] else row[0]  # å¦‚æœæ²¡æœ‰æè¿°ï¼Œä½¿ç”¨è¡¨å
                            })
                        
                        print(f"æˆåŠŸä½¿ç”¨ç¼–ç  {enc} ä»schema '{schema}' è·å–åˆ° {len(tables)} ä¸ªè¡¨ï¼ˆå«æè¿°ï¼‰")
                        return tables
                        
                except Exception as e:
                    print(f"ç¼–ç  {enc} å¤±è´¥: {str(e)}")
                    last_error = e
                    continue
            
            raise Exception(f"æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥ï¼Œæœ€åé”™è¯¯: {last_error}")
            
        except Exception as e:
            print(f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {str(e)}")
            raise e
    
    @staticmethod
    def get_table_fields(db_config, table_name):
        """è·å–è¡¨å­—æ®µä¿¡æ¯ï¼ˆåŒ…å«æè¿°ï¼‰"""
        try:
            # å°è¯•å¤šç§ç¼–ç è·å–å­—æ®µä¿¡æ¯
            encodings = ['utf8', 'gbk', 'latin1']
            last_error = None
            
            # è·å–schemaï¼Œé»˜è®¤ä¸ºpublic
            schema = db_config.get('schema', 'public')
            
            for enc in encodings:
                try:
                    connection_string = DatabaseService.get_connection_string(db_config, enc)
                    engine = create_engine(connection_string, connect_args={'client_encoding': enc})
                    
                    with engine.connect() as conn:
                        conn.execute(text(f"SET client_encoding = '{enc}'"))
                        
                        # æŸ¥è¯¢å­—æ®µåã€ç±»å‹å’Œæè¿°ï¼Œä½¿ç”¨åŠ¨æ€schema
                        query = """
                        SELECT 
                            a.attname as column_name,
                            pg_catalog.format_type(a.atttypid, a.atttypmod) as data_type,
                            a.attnotnull as not_null,
                            COALESCE(pg_catalog.col_description(c.oid, a.attnum), '') as column_description
                        FROM pg_catalog.pg_attribute a
                        JOIN pg_catalog.pg_class c ON a.attrelid = c.oid
                        JOIN pg_catalog.pg_namespace n ON c.relnamespace = n.oid
                        WHERE c.relname = :table_name
                        AND n.nspname = :schema
                        AND a.attnum > 0
                        AND NOT a.attisdropped
                        ORDER BY a.attnum
                        """
                        
                        result = conn.execute(text(query), {'table_name': table_name, 'schema': schema})
                        
                        fields = []
                        for row in result.fetchall():
                            field = {
                                'name': row[0],
                                'type': row[1],
                                'nullable': not row[2],
                                'description': row[3] if row[3] else row[0],  # å¦‚æœæ²¡æœ‰æè¿°ï¼Œä½¿ç”¨å­—æ®µå
                                'primary_key': False,
                                'default': None
                            }
                            fields.append(field)
                        
                        print(f"æˆåŠŸä½¿ç”¨ç¼–ç  {enc} ä»schema '{schema}' è·å–åˆ° {len(fields)} ä¸ªå­—æ®µï¼ˆå«æè¿°ï¼‰")
                        return fields
                        
                except Exception as e:
                    last_error = e
                    print(f"ä½¿ç”¨ç¼–ç  {enc} è·å–å­—æ®µå¤±è´¥: {str(e)}")
                    continue
            
            raise last_error
            
        except Exception as e:
            raise Exception(f"è·å–è¡¨å­—æ®µå¤±è´¥: {str(e)}")
    
    @staticmethod
    def read_data_in_batches(db_config, table_name, fields=None, batch_size=10000, max_rows=None, schema='public', filters=None, start_date=None, end_date=None, date_column='update_date'):
        """
        åˆ†æ‰¹è¯»å–å¤§æ•°æ®é›†ï¼Œé¿å…å†…å­˜æº¢å‡º
        
        Args:
            db_config: æ•°æ®åº“é…ç½®
            table_name: è¡¨å
            fields: å­—æ®µåˆ—è¡¨
            batch_size: æ¯æ‰¹æ¬¡å¤§å°
            max_rows: æœ€å¤§è¯»å–è¡Œæ•°ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼Œä½†ä¼šæ™ºèƒ½é‡‡æ ·ï¼‰
            schema: schemaåç§°
            filters: å­—æ®µè¿‡æ»¤æ¡ä»¶å­—å…¸ {field_name: value}
            start_date: å¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰ï¼Œç­›é€‰date_column >= start_date
            end_date: ç»“æŸæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰ï¼Œç­›é€‰date_column <= end_date
            date_column: ç”¨äºæ—¶é—´èŒƒå›´ç­›é€‰çš„åˆ—åï¼Œé»˜è®¤ä¸º'update_date'
            
        Returns:
            generator: è¿”å›DataFrameæ‰¹æ¬¡çš„ç”Ÿæˆå™¨
        """
        import logging
        logger = logging.getLogger(__name__)
        
        # å°è¯•å¤šç§ç¼–ç 
        encodings = ['utf8', 'gbk']
        last_error = None
        
        for encoding in encodings:
            try:
                logger.info(f"å°è¯•ä½¿ç”¨ç¼–ç  {encoding} åˆ†æ‰¹è¯»å–æ•°æ®: è¡¨={table_name}, æ‰¹æ¬¡å¤§å°={batch_size}, æœ€å¤§è¡Œæ•°={max_rows}")
                
                connection_string = DatabaseService.get_connection_string(db_config, encoding)
                engine = DatabaseService.create_engine(connection_string)
                
                # è®¾ç½®æ•°æ®åº“å®¢æˆ·ç«¯ç¼–ç 
                try:
                    with engine.connect() as test_conn:
                        if encoding == 'utf8':
                            test_conn.execute(text("SET client_encoding = 'UTF8'"))
                        elif encoding == 'gbk':
                            test_conn.execute(text("SET client_encoding = 'GBK'"))
                        logger.info(f"æˆåŠŸè®¾ç½®æ•°æ®åº“å®¢æˆ·ç«¯ç¼–ç ä¸º {encoding}")
                except Exception as enc_error:
                    logger.warning(f"è®¾ç½®ç¼–ç  {encoding} å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç : {str(enc_error)}")
                
                # å°è¯•è¯»å–ç¬¬ä¸€æ‰¹æ•°æ®éªŒè¯ç¼–ç æ˜¯å¦æ­£ç¡®
                # åˆ›å»ºç”Ÿæˆå™¨å¹¶å°è¯•è·å–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
                gen = DatabaseService._read_data_in_batches_with_engine(
                    engine, table_name, fields, batch_size, max_rows, schema, logger, filters, start_date, end_date, date_column
                )
                
                # æµ‹è¯•ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼ŒéªŒè¯ç¼–ç 
                try:
                    first_batch = next(gen)
                    # å¦‚æœæˆåŠŸï¼Œå…ˆyieldç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼Œç„¶åyieldå‰©ä½™çš„
                    def yield_all():
                        yield first_batch
                        for batch in gen:
                            yield batch
                    return yield_all()
                except StopIteration:
                    # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºç”Ÿæˆå™¨
                    return iter([])
                
            except UnicodeDecodeError as e:
                last_error = f"'{encoding}' codec can't decode: {str(e)}"
                logger.warning(f"ç¼–ç  {encoding} å¤±è´¥: {last_error}")
                try:
                    engine.dispose()
                except:
                    pass
                continue
            except Exception as e:
                last_error = str(e)
                logger.error(f"ä½¿ç”¨ç¼–ç  {encoding} è¯»å–æ•°æ®å¤±è´¥: {last_error}")
                try:
                    engine.dispose()
                except:
                    pass
                # å¦‚æœä¸æ˜¯ç¼–ç é—®é¢˜ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
                if 'decode' not in str(e).lower() and 'codec' not in str(e).lower():
                    raise
                continue
        
        # æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥
        raise Exception(f"æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥ï¼Œæœ€åé”™è¯¯: {last_error}")
    
    @staticmethod
    def _read_data_in_batches_with_engine(engine, table_name, fields=None, batch_size=10000, max_rows=None, schema='public', logger=None, filters=None, start_date=None, end_date=None, date_column='update_date'):
        """ä½¿ç”¨æŒ‡å®šçš„engineåˆ†æ‰¹è¯»å–æ•°æ®"""
        if logger is None:
            import logging
            logger = logging.getLogger(__name__)
        
        try:
            quoted_table_name = DatabaseService.quote_identifier(table_name)
            effective_schema = schema if (schema and isinstance(schema, str) and schema.strip()) else 'public'
            
            if effective_schema != 'public':
                quoted_schema = DatabaseService.quote_identifier(effective_schema)
                full_table_name = f"{quoted_schema}.{quoted_table_name}"
            else:
                full_table_name = quoted_table_name
            
            # --- æ„å»ºè¿‡æ»¤æ¡ä»¶ ---
            where_clause = ""
            conditions = []
            
            # å¤„ç†å­—æ®µè¿‡æ»¤æ¡ä»¶
            if filters and isinstance(filters, dict):
                for f_name, f_val in filters.items():
                    if f_name and f_val is not None:
                        q_field = DatabaseService.quote_identifier(f_name)
                        # ç®€å•é˜²æ³¨å…¥å¤„ç†
                        safe_val = str(f_val).replace("'", "''") 
                        conditions.append(f"{q_field} = '{safe_val}'")
            
            # å¤„ç†æ—¶é—´èŒƒå›´è¿‡æ»¤æ¡ä»¶
            if start_date or end_date:
                quoted_date_column = DatabaseService.quote_identifier(date_column)
                if start_date:
                    # ç®€å•é˜²æ³¨å…¥å¤„ç†
                    safe_start_date = str(start_date).replace("'", "''")
                    conditions.append(f"{quoted_date_column} >= '{safe_start_date}'")
                    logger.info(f"æ·»åŠ å¼€å§‹æ—¥æœŸè¿‡æ»¤: {quoted_date_column} >= '{safe_start_date}'")
                if end_date:
                    # ç®€å•é˜²æ³¨å…¥å¤„ç†
                    safe_end_date = str(end_date).replace("'", "''")
                    conditions.append(f"{quoted_date_column} <= '{safe_end_date}'")
                    logger.info(f"æ·»åŠ ç»“æŸæ—¥æœŸè¿‡æ»¤: {quoted_date_column} <= '{safe_end_date}'")
            
            if conditions:
                where_clause = "WHERE " + " AND ".join(conditions)
            # -------------------

            # [ä¼˜åŒ–] ç§»é™¤ SELECT COUNT(*) æŸ¥è¯¢ï¼Œç›´æ¥æŒ‰éœ€è¯»å–
            # æ—§é€»è¾‘ï¼šå…ˆCountå†é‡‡æ ·(Sampling)ï¼Œä¼šå¯¼è‡´å¤§è¡¨å¡æ­»ä¸”ä¸ç¬¦åˆ"é™åˆ¶æ•°æ®é‡"çš„ç›´è§‰
            # æ–°é€»è¾‘ï¼šç›´æ¥ LIMIT æ–¹å¼åˆ†æ‰¹è¯»å–å‰ N æ¡ (Sequential Reading)
            
            if fields:
                quoted_fields = [DatabaseService.quote_identifier(field) for field in fields]
                field_list = ', '.join(quoted_fields)
                base_query = f"SELECT {field_list} FROM {full_table_name} {where_clause}"
            else:
                base_query = f"SELECT * FROM {full_table_name} {where_clause}"
            
            offset = 0
            total_yielded = 0
            
            # å¾ªç¯è¯»å–ç›´åˆ°è¾¾åˆ° max_rows æˆ–æ•°æ®è¯»å®Œ
            while max_rows is None or total_yielded < max_rows:
                try:
                    # è®¡ç®—å½“å‰æ‰¹æ¬¡éœ€è¦è¯»å–çš„æ¡æ•°
                    current_limit = batch_size
                    if max_rows is not None:
                        remaining = max_rows - total_yielded
                        if remaining < batch_size:
                            current_limit = remaining
                    
                    # ç›´æ¥ä½¿ç”¨ LIMIT OFFSET è¿›è¡Œåˆ†æ‰¹è¯»å–
                    query = f"{base_query} LIMIT {current_limit} OFFSET {offset}"
                    
                    df_batch = pd.read_sql(query, engine)
                    
                    if df_batch.empty:
                        break
                    
                    rows_fetched = len(df_batch)
                    total_yielded += rows_fetched
                    offset += rows_fetched
                    
                    yield df_batch
                    
                    # å¦‚æœè¯»å–åˆ°çš„æ•°æ®å°‘äºè¯·æ±‚çš„é™åˆ¶ï¼Œè¯´æ˜æ•°æ®å·²ç»è¯»å®Œäº†
                    if rows_fetched < current_limit:
                        break
                        
                except Exception as batch_error:
                    logger.error(f"è¯»å–æ‰¹æ¬¡å¤±è´¥: {str(batch_error)}")
                    raise
            
            logger.info(f"åˆ†æ‰¹è¯»å–å®Œæˆï¼Œå…±è¯»å– {total_yielded} è¡Œ")
            engine.dispose()
            
        except Exception as e:
            logger.error(f"åˆ†æ‰¹è¯»å–æ•°æ®å¤±è´¥: {str(e)}")
            raise Exception(f"åˆ†æ‰¹è¯»å–æ•°æ®å¤±è´¥: {str(e)}")
    
    @staticmethod
    def preview_data(db_config, table_name, fields=None, limit=100):
        """é¢„è§ˆæ•°æ®"""
        # ä¿è¯try/exceptç»“æ„æ­£ç¡®
        try:
            print("æµ‹è¯•åŸºæœ¬æ•°æ®åº“è¿æ¥...")
            if not DatabaseService.test_connection(db_config):
                raise Exception("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“é…ç½®")
            print("åŸºæœ¬æ•°æ®åº“è¿æ¥æ­£å¸¸")
            
            # åªä½¿ç”¨æ”¯æŒä¸­æ–‡çš„ç¼–ç ï¼Œç§»é™¤latin-1
            encodings = ['utf8', 'gbk']
            last_error = None
            
            for enc in encodings:
                try:
                    print(f"å°è¯•ä½¿ç”¨ç¼–ç  {enc} é¢„è§ˆæ•°æ®...")
                    connection_string = DatabaseService.get_connection_string(db_config, enc)
                    
                    # ç®€åŒ–è¿æ¥å‚æ•°ï¼Œé¿å…ç¼–ç å†²çª
                    engine = create_engine(
                        connection_string, 
                        pool_pre_ping=True,
                        echo=False
                    )
                    
                    with engine.connect() as conn:
                        # è®¾ç½®æ•°æ®åº“å®¢æˆ·ç«¯ç¼–ç 
                        try:
                            if enc == 'utf8':
                                conn.execute(text("SET client_encoding = 'UTF8'"))
                            elif enc == 'gbk':
                                conn.execute(text("SET client_encoding = 'GBK'"))
                        except Exception as enc_error:
                            print(f"è®¾ç½®ç¼–ç  {enc} å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç : {str(enc_error)}")
                        
                        # ä½¿ç”¨å¼•å·åŒ…è£…è¡¨åå’Œå­—æ®µå
                        quoted_table_name = DatabaseService.quote_identifier(table_name)
                        
                        # æ„å»ºå®Œæ•´çš„è¡¨åï¼ˆåŒ…å«schemaï¼‰
                        schema = db_config.get('schema', 'public')
                        if schema and schema != 'public':
                            quoted_schema = DatabaseService.quote_identifier(schema)
                            full_table_name = f"{quoted_schema}.{quoted_table_name}"
                        else:
                            full_table_name = quoted_table_name
                        
                        if fields:
                            quoted_fields = [DatabaseService.quote_identifier(field) for field in fields]
                            field_list = ', '.join(quoted_fields)
                            if limit is None:
                                query = f"SELECT {field_list} FROM {full_table_name}"
                            else:
                                query = f"SELECT {field_list} FROM {full_table_name} LIMIT {limit}"
                        else:
                            if limit is None:
                                query = f"SELECT * FROM {full_table_name}"
                            else:
                                query = f"SELECT * FROM {full_table_name} LIMIT {limit}"
                        
                        print(f"æ‰§è¡ŒæŸ¥è¯¢: {query}")
                        
                        # å…ˆæµ‹è¯•è¿æ¥æ˜¯å¦æ­£å¸¸
                        try:
                            test_result = conn.execute(text("SELECT 1"))
                            test_result.fetchone()
                            print("æ•°æ®åº“è¿æ¥æ­£å¸¸")
                        except Exception as conn_error:
                            print(f"æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥: {str(conn_error)}")
                            raise conn_error
                        
                        # æµ‹è¯•è¡¨æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨ä¸åŒºåˆ†å¤§å°å†™çš„æŸ¥è¯¢ï¼‰
                        try:
                            table_check = conn.execute(text(f"SELECT EXISTS (SELECT FROM information_schema.tables WHERE LOWER(table_name) = LOWER('{table_name}'))"))
                            table_exists = table_check.fetchone()[0]
                            if not table_exists:
                                raise Exception(f"è¡¨ {table_name} ä¸å­˜åœ¨")
                            print(f"è¡¨ {table_name} å­˜åœ¨")
                        except Exception as table_error:
                            print(f"è¡¨æ£€æŸ¥å¤±è´¥: {str(table_error)}")
                            raise table_error
                        
                        # ç›´æ¥ä½¿ç”¨pandasè¯»å–ï¼Œä¸ä¼ é€’encodingå‚æ•°
                        df = pd.read_sql(query, engine)
                        
                        if df is not None and len(df) > 0:
                            print(f"æˆåŠŸè·å– {len(df)} è¡Œæ•°æ®")
                            
                            # æ”¹è¿›ä¸­æ–‡å­—ç¬¦å¤„ç†
                            for col in df.select_dtypes(include=['object']).columns:
                                df[col] = df[col].astype(str).apply(
                                    lambda x: x if x == 'nan' else (
                                        x.encode('utf-8', errors='replace').decode('utf-8') 
                                        if isinstance(x, str) else str(x)
                                    )
                                )
                            
                            return df.to_dict('records')
                        elif df is not None:
                            print("æŸ¥è¯¢ç»“æœä¸ºç©º")
                            return []
                        else:
                            print(f"ç¼–ç  {enc} è¯»å–å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç¼–ç ")
                            continue
                            
                except Exception as e:
                    print(f"ç¼–ç  {enc} é¢„è§ˆæ•°æ®å¤±è´¥: {str(e)}")
                    last_error = e
                    continue
            
            if last_error:
                raise Exception(f"æ‰€æœ‰ç¼–ç å°è¯•éƒ½å¤±è´¥äº†ï¼Œæœ€åé”™è¯¯: {str(last_error)}")
            else:
                raise Exception("æ‰€æœ‰ç¼–ç å°è¯•éƒ½å¤±è´¥äº†ï¼Œä½†æ²¡æœ‰æ•è·åˆ°å…·ä½“é”™è¯¯")
                
        except Exception as e:
            raise Exception(f"é¢„è§ˆæ•°æ®å¤±è´¥: {str(e)}")

    @staticmethod
    def get_data_statistics(db_config, table_name, fields):
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        try:
            print("æµ‹è¯•åŸºæœ¬æ•°æ®åº“è¿æ¥...")
            if not DatabaseService.test_connection(db_config):
                raise Exception("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“é…ç½®")
            print("åŸºæœ¬æ•°æ®åº“è¿æ¥æ­£å¸¸")
            
            # åªä½¿ç”¨æ”¯æŒä¸­æ–‡çš„ç¼–ç ï¼Œç§»é™¤latin-1
            encodings = ['utf8', 'gbk']
            last_error = None
            
            for enc in encodings:
                try:
                    print(f"å°è¯•ä½¿ç”¨ç¼–ç  {enc} è·å–ç»Ÿè®¡ä¿¡æ¯...")
                    connection_string = DatabaseService.get_connection_string(db_config, enc)
                    
                    # ç®€åŒ–è¿æ¥å‚æ•°ï¼Œé¿å…ç¼–ç å†²çª
                    engine = create_engine(
                        connection_string, 
                        pool_pre_ping=True,
                        echo=False
                    )
                    
                    with engine.connect() as conn:
                        # è®¾ç½®æ•°æ®åº“å®¢æˆ·ç«¯ç¼–ç 
                        try:
                            if enc == 'utf8':
                                conn.execute(text("SET client_encoding = 'UTF8'"))
                            elif enc == 'gbk':
                                conn.execute(text("SET client_encoding = 'GBK'"))
                        except Exception as enc_error:
                            print(f"è®¾ç½®ç¼–ç  {enc} å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç : {str(enc_error)}")
                        
                        # ä½¿ç”¨å¼•å·åŒ…è£…è¡¨åå’Œå­—æ®µå
                        quoted_table_name = DatabaseService.quote_identifier(table_name)
                        
                        # æ„å»ºå®Œæ•´çš„è¡¨åï¼ˆåŒ…å«schemaï¼‰
                        schema = db_config.get('schema', 'public')
                        if schema and schema != 'public':
                            quoted_schema = DatabaseService.quote_identifier(schema)
                            full_table_name = f"{quoted_schema}.{quoted_table_name}"
                        else:
                            full_table_name = quoted_table_name
                        
                        quoted_fields = [DatabaseService.quote_identifier(field) for field in fields]
                        field_list = ', '.join(quoted_fields)
                        query = f"SELECT {field_list} FROM {full_table_name}"
                        print(f"æ‰§è¡Œç»Ÿè®¡æŸ¥è¯¢: {query}")
                        
                        # ç›´æ¥ä½¿ç”¨pandasè¯»å–ï¼Œä¸ä¼ é€’encodingå‚æ•°
                        df = pd.read_sql(query, engine)
                        
                        if df is not None and len(df) > 0:
                            print(f"æˆåŠŸè·å– {len(df)} è¡Œæ•°æ®ç”¨äºç»Ÿè®¡")
                            
                            # æ”¹è¿›ä¸­æ–‡å­—ç¬¦å¤„ç†
                            for col in df.select_dtypes(include=['object']).columns:
                                df[col] = df[col].astype(str).apply(
                                    lambda x: x if x == 'nan' else (
                                        x.encode('utf-8', errors='replace').decode('utf-8') 
                                        if isinstance(x, str) else str(x)
                                    )
                                )
                            
                            statistics = {}
                            for field in fields:
                                if field in df.columns:
                                    statistics[field] = {
                                        'count': int(df[field].count()),
                                        'mean': float(df[field].mean()) if pd.api.types.is_numeric_dtype(df[field]) else None,
                                        'std': float(df[field].std()) if pd.api.types.is_numeric_dtype(df[field]) else None,
                                        'min': float(df[field].min()) if pd.api.types.is_numeric_dtype(df[field]) else None,
                                        'max': float(df[field].max()) if pd.api.types.is_numeric_dtype(df[field]) else None
                                    }
                                else:
                                    statistics[field] = {
                                        'count': 0,
                                        'mean': None,
                                        'std': None,
                                        'min': None,
                                        'max': None
                                    }
                            return statistics
                        elif df is not None:
                            print("æŸ¥è¯¢ç»“æœä¸ºç©ºï¼Œè¿”å›ç©ºç»Ÿè®¡ä¿¡æ¯")
                            return {field: {'count': 0, 'mean': None, 'std': None, 'min': None, 'max': None} for field in fields}
                        else:
                            print(f"ç¼–ç  {enc} è¯»å–å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç¼–ç ")
                            continue
                            
                except Exception as e:
                    print(f"ç¼–ç  {enc} è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
                    last_error = e
                    continue
            
            if last_error:
                raise Exception(f"æ‰€æœ‰ç¼–ç å°è¯•éƒ½å¤±è´¥äº†ï¼Œæœ€åé”™è¯¯: {str(last_error)}")
            else:
                raise Exception("æ‰€æœ‰ç¼–ç å°è¯•éƒ½å¤±è´¥äº†ï¼Œä½†æ²¡æœ‰æ•è·åˆ°å…·ä½“é”™è¯¯")
                
        except Exception as e:
            raise Exception(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
    
    @staticmethod
    def save_data_source(name, db_type, host, port, database, username, password, status=False):
        data_source = DataSource(
            name=name,
            db_type=db_type,
            host=host,
            port=port,
            database=database,
            username=username,
            password=password,
            status=status
        )
        db.session.add(data_source)
        db.session.commit()
        return data_source
    
    @staticmethod
    def get_data_sources():
        """è·å–æ‰€æœ‰æ•°æ®æº"""
        try:
            print("å¼€å§‹è·å–æ•°æ®æºåˆ—è¡¨...")
            # åªè¿”å›æ´»è·ƒçš„æ•°æ®æºï¼Œè¿™æ ·æ›´ç¬¦åˆä¸šåŠ¡é€»è¾‘
            sources = DataSource.query.filter_by(is_active=True).all()
            print(f"æˆåŠŸè·å– {len(sources)} ä¸ªæ´»è·ƒæ•°æ®æº")
            
            result = []
            for source in sources:
                try:
                    source_dict = source.to_dict()
                    result.append(source_dict)
                except Exception as e:
                    print(f"è½¬æ¢æ•°æ®æº {source.id} å¤±è´¥: {str(e)}")
                    # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªæ•°æ®æº
                    continue
            
            print(f"æˆåŠŸè½¬æ¢ {len(result)} ä¸ªæ•°æ®æº")
            return result
            
        except Exception as e:
            print(f"è·å–æ•°æ®æºå¤±è´¥: {str(e)}")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            raise Exception(f"è·å–æ•°æ®æºå¤±è´¥: {str(e)}")
    
    @staticmethod
    def load_cnooc_config():
        """åŠ è½½CNOOCæ•°æ®åº“é…ç½®"""
        try:
            config = configparser.ConfigParser()
            config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'config', 'db_config.ini')
            config.read(config_path)
            
            if 'POSTGRES_DB' in config:
                db_config = config['POSTGRES_DB']
                return {
                    'host': db_config['host'],
                    'port': int(db_config['port']),
                    'database': db_config['database'],
                    'username': db_config['username'],
                    'password': db_config['password'],
                    'client_encoding': db_config.get('client_encoding', 'utf8'),
                    'db_type': 'postgresql'
                }
            else:
                raise Exception("é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°POSTGRES_DBé…ç½®")
        except Exception as e:
            raise Exception(f"åŠ è½½CNOOCé…ç½®å¤±è´¥: {str(e)}")
    
    @staticmethod
    def get_distinct_values(db_config, table_name, field_name, limit=1000):
        """è·å–æŒ‡å®šå­—æ®µçš„ä¸åŒå€¼"""
        try:
            print(f"è·å–å­—æ®µ {field_name} åœ¨è¡¨ {table_name} ä¸­çš„ä¸åŒå€¼...")
            
            # æµ‹è¯•æ•°æ®åº“è¿æ¥
            if not DatabaseService.test_connection(db_config):
                raise Exception("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“é…ç½®")
            
            # æ”¯æŒä¸­æ–‡çš„ç¼–ç 
            encodings = ['utf8', 'gbk']
            last_error = None
            
            for enc in encodings:
                try:
                    print(f"å°è¯•ä½¿ç”¨ç¼–ç  {enc} è·å–ä¸åŒå€¼...")
                    connection_string = DatabaseService.get_connection_string(db_config, enc)
                    
                    engine = create_engine(
                        connection_string, 
                        pool_pre_ping=True,
                        echo=False
                    )
                    
                    with engine.connect() as conn:
                        # è®¾ç½®æ•°æ®åº“å®¢æˆ·ç«¯ç¼–ç 
                        try:
                            if enc == 'utf8':
                                conn.execute(text("SET client_encoding = 'UTF8'"))
                            elif enc == 'gbk':
                                conn.execute(text("SET client_encoding = 'GBK'"))
                        except Exception as enc_error:
                            print(f"è®¾ç½®ç¼–ç  {enc} å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç : {str(enc_error)}")
                        
                        # ä½¿ç”¨å¼•å·åŒ…è£…è¡¨åå’Œå­—æ®µå
                        quoted_table_name = DatabaseService.quote_identifier(table_name)
                        quoted_field_name = DatabaseService.quote_identifier(field_name)
                        
                        # æ„å»ºå®Œæ•´çš„è¡¨åï¼ˆåŒ…å«schemaï¼‰
                        schema = db_config.get('schema', 'public')
                        if schema and schema != 'public':
                            quoted_schema = DatabaseService.quote_identifier(schema)
                            full_table_name = f"{quoted_schema}.{quoted_table_name}"
                        else:
                            full_table_name = quoted_table_name
                        
                        # æ„å»ºæŸ¥è¯¢è¯­å¥è·å–ä¸åŒå€¼
                        query = f"SELECT DISTINCT {quoted_field_name} FROM {full_table_name} WHERE {quoted_field_name} IS NOT NULL ORDER BY {quoted_field_name} LIMIT {limit}"
                        
                        print(f"æ‰§è¡ŒæŸ¥è¯¢: {query}")
                        result = conn.execute(text(query))
                        
                        # è·å–æ‰€æœ‰ä¸åŒå€¼
                        distinct_values = [row[0] for row in result.fetchall()]
                        
                        print(f"æˆåŠŸè·å– {len(distinct_values)} ä¸ªä¸åŒå€¼ï¼Œä½¿ç”¨ç¼–ç : {enc}")
                        return distinct_values
                        
                except Exception as e:
                    last_error = e
                    print(f"ä½¿ç”¨ç¼–ç  {enc} è·å–ä¸åŒå€¼å¤±è´¥: {str(e)}")
                    continue
            
            # æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥äº†
            raise Exception(f"æ‰€æœ‰ç¼–ç å°è¯•éƒ½å¤±è´¥äº†ï¼Œæœ€åé”™è¯¯: {str(last_error)}")
            
        except Exception as e:
            print(f"è·å–å­—æ®µä¸åŒå€¼å¤±è´¥: {str(e)}")
            raise Exception(f"è·å–å­—æ®µä¸åŒå€¼å¤±è´¥: {str(e)}")
    
    @staticmethod
    def preview_data_with_filter(db_config, table_name, fields=None, limit=100, company_field=None, company_value=None):
        """é¢„è§ˆæ•°æ®ï¼ˆæ”¯æŒåˆ†å…¬å¸è¿‡æ»¤ï¼‰"""
        try:
            print(f"é¢„è§ˆæ•°æ®: è¡¨={table_name}, å­—æ®µ={fields}, åˆ†å…¬å¸å­—æ®µ={company_field}, åˆ†å…¬å¸å€¼={company_value}")
            
            # æµ‹è¯•æ•°æ®åº“è¿æ¥
            if not DatabaseService.test_connection(db_config):
                raise Exception("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“é…ç½®")
            
            # æ”¯æŒä¸­æ–‡çš„ç¼–ç 
            encodings = ['utf8', 'gbk']
            last_error = None
            
            for enc in encodings:
                try:
                    print(f"å°è¯•ä½¿ç”¨ç¼–ç  {enc} é¢„è§ˆæ•°æ®...")
                    connection_string = DatabaseService.get_connection_string(db_config, enc)
                    
                    engine = create_engine(
                        connection_string, 
                        pool_pre_ping=True,
                        echo=False
                    )
                    
                    with engine.connect() as conn:
                        # è®¾ç½®æ•°æ®åº“å®¢æˆ·ç«¯ç¼–ç 
                        try:
                            if enc == 'utf8':
                                conn.execute(text("SET client_encoding = 'UTF8'"))
                            elif enc == 'gbk':
                                conn.execute(text("SET client_encoding = 'GBK'"))
                        except Exception as enc_error:
                            print(f"è®¾ç½®ç¼–ç  {enc} å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç : {str(enc_error)}")
                        
                        # ä½¿ç”¨å¼•å·åŒ…è£…è¡¨åå’Œå­—æ®µå
                        quoted_table_name = DatabaseService.quote_identifier(table_name)
                        
                        # æ„å»ºå®Œæ•´çš„è¡¨åï¼ˆåŒ…å«schemaï¼‰
                        schema = db_config.get('schema', 'public')
                        if schema and schema != 'public':
                            quoted_schema = DatabaseService.quote_identifier(schema)
                            full_table_name = f"{quoted_schema}.{quoted_table_name}"
                        else:
                            full_table_name = quoted_table_name
                        
                        # æ„å»ºå­—æ®µåˆ—è¡¨
                        if fields:
                            quoted_fields = [DatabaseService.quote_identifier(field) for field in fields]
                            field_list = ', '.join(quoted_fields)
                        else:
                            field_list = '*'
                        
                        # æ„å»ºåŸºæœ¬æŸ¥è¯¢
                        query = f"SELECT {field_list} FROM {full_table_name}"
                        
                        # æ·»åŠ åˆ†å…¬å¸è¿‡æ»¤æ¡ä»¶
                        if company_field and company_value:
                            quoted_company_field = DatabaseService.quote_identifier(company_field)
                            # ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢é˜²æ­¢SQLæ³¨å…¥ï¼ŒåŒæ—¶å¤„ç†å­—ç¬¦ä¸²è½¬ä¹‰
                            escaped_value = company_value.replace("'", "''")  # SQLå­—ç¬¦ä¸²è½¬ä¹‰
                            query += f" WHERE {quoted_company_field} = '{escaped_value}'"
                        
                        # æ·»åŠ é™åˆ¶
                        if limit is not None:
                            query += f" LIMIT {limit}"
                        
                        print(f"æ‰§è¡ŒæŸ¥è¯¢: {query}")
                        
                        # ä½¿ç”¨pandasè¯»å–æ•°æ®
                        df = pd.read_sql(query, conn)
                        
                        print(f"æˆåŠŸè·å– {len(df)} è¡Œæ•°æ®ï¼Œä½¿ç”¨ç¼–ç : {enc}")
                        return df.to_dict('records')
                        
                except Exception as e:
                    last_error = e
                    print(f"ä½¿ç”¨ç¼–ç  {enc} é¢„è§ˆæ•°æ®å¤±è´¥: {str(e)}")
                    continue
            
            # æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥äº†
            raise Exception(f"æ‰€æœ‰ç¼–ç å°è¯•éƒ½å¤±è´¥äº†ï¼Œæœ€åé”™è¯¯: {str(last_error)}")
            
        except Exception as e:
            print(f"é¢„è§ˆæ•°æ®å¤±è´¥: {str(e)}")
            raise Exception(f"é¢„è§ˆæ•°æ®å¤±è´¥: {str(e)}")
    
    @staticmethod
    def get_tag_data(db_config, table_name, tag_code, tag_field_name='tag_code', limit=300, start_time=None, end_time=None, date_field='tag_time'):
        """è·å–TAGæ•°æ®ï¼ˆç”¨äºç”Ÿäº§æ•°æ®è´¨æ£€çš„è¶‹åŠ¿å›¾ï¼‰
        
        å¼ºåˆ¶å®æ–½æ•°æ®é‡é™åˆ¶ï¼Œä¿æŠ¤ç”Ÿäº§ç¯å¢ƒæ•°æ®åº“
        
        Args:
            db_config: æ•°æ®åº“é…ç½®
            table_name: è¡¨å
            tag_code: TAGä»£ç /å­—æ®µå€¼
            tag_field_name: TAGå­—æ®µåï¼ˆé»˜è®¤'tag_code'ï¼Œæ”¯æŒåŠ¨æ€å­—æ®µï¼‰
            limit: æ•°æ®é‡é™åˆ¶ï¼ˆæœ€å¤§300ï¼‰
            start_time: å¼€å§‹æ—¶é—´ï¼ˆå¯é€‰ï¼Œæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆå¯é€‰ï¼Œæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼‰
            date_field: æ—¶é—´å­—æ®µåï¼ˆé»˜è®¤'tag_time'ï¼‰
        
        Returns:
            list: TAGæ•°æ®åˆ—è¡¨ï¼Œæ¯æ¡è®°å½•åŒ…å« tag_code, tag_time, tag_value
        """
        try:
            # å¼ºåˆ¶é™åˆ¶ï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
            # [ä¼˜åŒ–] ç¡®ä¿ limit æ˜¯æœ‰æ•ˆçš„æ•´æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™é»˜è®¤ä¸º2000
            if limit is None:
                limit = 2000
            else:
                limit = min(int(limit), 2000)

            print(f"ğŸ”’ æŸ¥è¯¢TAGæ•°æ®: {tag_field_name}={tag_code}, limit={limit}")
            
            # è·å–æ•°æ®åº“è¿æ¥
            connection_string = DatabaseService.get_connection_string(db_config, 'utf8')
            engine = DatabaseService.create_engine(connection_string)
            
            # æ„å»ºè¡¨å
            schema = db_config.get('schema', 'public')
            quoted_table = DatabaseService.quote_identifier(table_name)
            if schema and schema != 'public':
                quoted_schema = DatabaseService.quote_identifier(schema)
                full_table = f"{quoted_schema}.{quoted_table}"
            else:
                full_table = quoted_table
            
            # å¼•ç”¨å­—æ®µå
            quoted_tag_field = DatabaseService.quote_identifier(tag_field_name)
            quoted_date_field = DatabaseService.quote_identifier(date_field)
            
            # æ„å»ºWHEREå­å¥
            where_clauses = [f"{quoted_tag_field} = '{tag_code}'"]
            
            if start_time:
                where_clauses.append(f"{quoted_date_field} >= '{start_time}'")
            if end_time:
                where_clauses.append(f"{quoted_date_field} <= '{end_time}'")
            
            where_clause = " AND ".join(where_clauses)
            
            # æ„å»ºæŸ¥è¯¢ï¼ˆä½¿ç”¨å€’åºç´¢å¼•ï¼Œè·å–æœ€æ–°æ•°æ®ï¼‰
            # å°†é€‰ä¸­çš„å­—æ®µåˆ«åä¸º tag_code ä»¥ä¿æŒæ¥å£ä¸€è‡´æ€§
            query = f"""
                SELECT {quoted_tag_field} as tag_code, {quoted_date_field} as tag_time, tag_value
                FROM {full_table}
                WHERE {where_clause}
                ORDER BY {quoted_date_field} DESC
                LIMIT {limit}
            """
            
            print(f"æ‰§è¡ŒæŸ¥è¯¢: {query}")
            
            # æ‰§è¡ŒæŸ¥è¯¢
            with engine.connect() as conn:
                df = pd.read_sql(text(query), conn)
            
            # æŒ‰æ—¶é—´æ­£åºæ’åˆ—ï¼ˆå‰ç«¯å±•ç¤ºéœ€è¦ï¼‰
            df = df.sort_values('tag_time')
            
            print(f"âœ… æˆåŠŸè·å– {len(df)} æ¡TAGæ•°æ®")
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            return df.to_dict('records')
            
        except Exception as e:
            print(f"âŒ è·å–TAGæ•°æ®å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            raise Exception(f"è·å–TAGæ•°æ®å¤±è´¥: {str(e)}")
    
    @staticmethod
    def detect_anomalies(db_config, table_name, tag_code, tag_field_name='tag_code',
                        gap_thres=60, win_sec=300, z_win=50, z_thres=2.0,
                        limit=10000, start_time=None, end_time=None, date_field='tag_time'):
        """æ£€æµ‹ç”Ÿäº§æ•°æ®å¼‚å¸¸ï¼ˆæ•°æ®ä¸¢å¤±ã€æ–­æµã€æ•°å€¼å¼‚å¸¸ï¼‰
        
        å¼ºåˆ¶å®æ–½æ•°æ®é‡é™åˆ¶ï¼Œä¿æŠ¤ç”Ÿäº§ç¯å¢ƒæ•°æ®åº“
        
        Args:
            db_config: æ•°æ®åº“é…ç½®
            table_name: è¡¨å
            tag_code: TAGä»£ç /å­—æ®µå€¼
            tag_field_name: TAGå­—æ®µåï¼ˆé»˜è®¤'tag_code'ï¼Œæ”¯æŒåŠ¨æ€å­—æ®µï¼‰
            gap_thres: æ•°æ®ä¸¢å¤±é˜ˆå€¼ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤60ç§’
            win_sec: æ–­æµæ£€æµ‹çª—å£ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤300ç§’ï¼ˆ5åˆ†é’Ÿï¼‰
            z_win: Z-Scoreçª—å£å¤§å°ï¼Œé»˜è®¤50
            z_win: Z-Scoreé˜ˆå€¼ï¼Œé»˜è®¤2.0
            limit: æ•°æ®é‡é™åˆ¶ï¼ˆæœ€å¤§50000ï¼‰
            start_time: å¼€å§‹æ—¶é—´ï¼ˆå¯é€‰ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆå¯é€‰ï¼‰
            date_field: æ—¶é—´å­—æ®µåï¼ˆé»˜è®¤'tag_time'ï¼‰
        
        Returns:
            dict: åŒ…å« anomalies_listï¼ˆå¼‚å¸¸åˆ—è¡¨ï¼‰ å’Œ chart_dataï¼ˆå›¾è¡¨æ•°æ®ï¼‰
        """
        import numpy as np
        from datetime import datetime
        
        try:
            # å¼ºåˆ¶é™åˆ¶ï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
            MAX_LIMIT = 50000

            if limit is None:
                limit = MAX_LIMIT
                print(f"âš ï¸  å‰ç«¯è¯·æ±‚å…¨é‡æ•°æ®ï¼Œå¼ºåˆ¶é™åˆ¶ä¸º {MAX_LIMIT} æ¡ä»¥ä¿æŠ¤æ•°æ®åº“")
            else:
                limit = min(int(limit), MAX_LIMIT)

            print(f"ğŸ”’ å¼‚å¸¸æ£€æµ‹: {tag_field_name}={tag_code}, limit={limit}, "
                  f"gap_thres={gap_thres}s, win_sec={win_sec}s, z_win={z_win}, z_thres={z_thres}")
            
            # 1. è·å–æ•°æ®ï¼ˆä½¿ç”¨æ›´å¤§çš„limitç”¨äºåˆ†æï¼‰
            tag_data = DatabaseService.get_tag_data(
                db_config, table_name, tag_code, tag_field_name,
                limit=limit,
                start_time=start_time,
                end_time=end_time,
                date_field=date_field
            )
            
            if not tag_data or len(tag_data) == 0:
                print("âš ï¸  æœªæŸ¥è¯¢åˆ°æ•°æ®")
                return {
                    'anomalies_list': [],
                    'chart_data': []
                }
            
            print(f"ğŸ“Š å¼€å§‹åˆ†æ {len(tag_data)} æ¡æ•°æ®...")
            
            # 2. æ•°æ®å‡†å¤‡
            df = pd.DataFrame(tag_data)
            df['tag_time'] = pd.to_datetime(df['tag_time'])
            df = df.sort_values('tag_time').reset_index(drop=True)
            df['tag_value'] = pd.to_numeric(df['tag_value'], errors='coerce')
            
            anomalies = []
            
            # 3. æ£€æµ‹æ•°æ®ä¸¢å¤±ï¼ˆæ—¶é—´é—´éš”è¶…è¿‡é˜ˆå€¼ï¼‰
            print("ğŸ” æ£€æµ‹æ•°æ®ä¸¢å¤±...")
            for i in range(1, len(df)):
                time_gap = (df.iloc[i]['tag_time'] - df.iloc[i-1]['tag_time']).total_seconds()
                if time_gap > gap_thres:
                    anomalies.append({
                        'code': tag_code,
                        'type': 'æ•°æ®ä¸¢å¤±',
                        'timestamp': df.iloc[i]['tag_time'].strftime('%Y-%m-%d %H:%M:%S'),
                        'value': None,
                        'details': f'æ•°æ®ç¼ºå¤± {int(time_gap)} ç§’ï¼ˆé˜ˆå€¼={gap_thres}ç§’ï¼‰',
                        # 'row_index': i + 1,
                        'time_range': [
                            df.iloc[i-1]['tag_time'].strftime('%Y-%m-%d %H:%M:%S'),
                            df.iloc[i]['tag_time'].strftime('%Y-%m-%d %H:%M:%S')
                        ]
                    })
            
            # 4. æ£€æµ‹æ•°æ®æ–­æµï¼ˆçª—å£å†…æ•°æ®ç‚¹è¿‡å°‘ï¼‰
            print("ğŸ” æ£€æµ‹æ•°æ®æ–­æµ...")
            # å°†win_secè½¬æ¢ä¸ºæ•°æ®ç‚¹æ•°é‡ä¼°ç®—
            if len(df) >= 2:
                avg_interval = (df.iloc[-1]['tag_time'] - df.iloc[0]['tag_time']).total_seconds() / len(df)
                expected_points_in_window = max(int(win_sec / avg_interval), 1) if avg_interval > 0 else 1
                
                # ä½¿ç”¨æ»šåŠ¨çª—å£æ£€æµ‹
                if len(df) >= expected_points_in_window:
                    df['rolling_count'] = df['tag_value'].rolling(
                        window=expected_points_in_window, 
                        min_periods=1
                    ).count()
                    
                    # æ–­æµåˆ¤æ–­ï¼šçª—å£å†…æœ‰æ•ˆæ•°æ®ç‚¹å°‘äºæœŸæœ›å€¼çš„50%
                    zero_flow_threshold = expected_points_in_window * 0.5
                    zero_flow = df[df['rolling_count'] < zero_flow_threshold]
                    
                    for idx, row in zero_flow.iterrows():
                        anomalies.append({
                            'code': tag_code,
                            'type': 'æ•°æ®æ–­æµ',
                            'timestamp': row['tag_time'].strftime('%Y-%m-%d %H:%M:%S'),
                            'value': float(row['tag_value']) if not pd.isna(row['tag_value']) else None,
                            'details': f'çª—å£å†…æ•°æ®ç‚¹æ•° {int(row["rolling_count"])} < æœŸæœ›å€¼ {int(zero_flow_threshold)}'
                            # 'row_index': idx + 1,
                        })
            
            # 5. æ£€æµ‹æ•°å€¼å¼‚å¸¸ï¼ˆZ-Scoreæ–¹æ³•ï¼‰
            print("ğŸ” æ£€æµ‹æ•°å€¼å¼‚å¸¸...")
            df_clean = df.dropna(subset=['tag_value'])
            
            if len(df_clean) >= z_win:
                # è®¡ç®—æ»šåŠ¨ç»Ÿè®¡é‡
                df_clean['rolling_mean'] = df_clean['tag_value'].rolling(
                    window=z_win, 
                    min_periods=1
                ).mean()
                df_clean['rolling_std'] = df_clean['tag_value'].rolling(
                    window=z_win, 
                    min_periods=1
                ).std()
                
                # è®¡ç®—Z-Scoreï¼ˆé¿å…é™¤ä»¥0ï¼‰
                df_clean['z_score'] = np.abs(
                    (df_clean['tag_value'] - df_clean['rolling_mean']) / 
                    (df_clean['rolling_std'] + 1e-10)
                )
                
                # æ‰¾å‡ºå¼‚å¸¸å€¼
                outliers = df_clean[df_clean['z_score'] > z_thres]
                
                for idx, row in outliers.iterrows():
                    anomalies.append({
                        'code': tag_code,
                        'type': 'æ•°æ®å¼‚å¸¸',
                        'timestamp': row['tag_time'].strftime('%Y-%m-%d %H:%M:%S'),
                        'value': float(row['tag_value']),
                        'details': f'Z-Score = {row["z_score"]:.2f} (é˜ˆå€¼={z_thres})'
                        # 'row_index': idx + 1,
                    })
            
            # 6. ç”Ÿæˆå›¾è¡¨æ•°æ®
            print("ğŸ“ˆ ç”Ÿæˆå›¾è¡¨æ•°æ®...")
            chart_data = []
            anomaly_timestamps = {a['timestamp']: a['type'] for a in anomalies}
            
            for idx, row in df.iterrows():
                timestamp_str = row['tag_time'].strftime('%Y-%m-%d %H:%M:%S')
                chart_data.append({
                    'tag_time': timestamp_str,
                    'tag_value': float(row['tag_value']) if not pd.isna(row['tag_value']) else None,
                    'anomaly_type': anomaly_timestamps.get(timestamp_str)
                })
            
            print(f"âœ… å¼‚å¸¸æ£€æµ‹å®Œæˆ: å‘ç° {len(anomalies)} ä¸ªå¼‚å¸¸")
            print(f"   - æ•°æ®ä¸¢å¤±: {sum(1 for a in anomalies if a['type'] == 'æ•°æ®ä¸¢å¤±')} ä¸ª")
            print(f"   - æ•°æ®æ–­æµ: {sum(1 for a in anomalies if a['type'] == 'æ•°æ®æ–­æµ')} ä¸ª")
            print(f"   - æ•°å€¼å¼‚å¸¸: {sum(1 for a in anomalies if a['type'] == 'æ•°æ®å¼‚å¸¸')} ä¸ª")
            
            return {
                'anomalies_list': anomalies,
                'chart_data': chart_data
            }
            
        except Exception as e:
            print(f"âŒ å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            raise Exception(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}")
    
    @staticmethod
    def get_well_parameter_sequence(db_config, table_name, well_id, parameter, 
                                     limit=10000, start_date=None, end_date=None, date_field=None):
        """è·å–äº•å‚æ•°åºåˆ—æ•°æ®ï¼ˆç”¨äºLSTMå¼‚å¸¸æ£€æµ‹ï¼‰
        
        å¼ºåˆ¶å®æ–½æ•°æ®é‡é™åˆ¶ï¼Œä¿æŠ¤ç”Ÿäº§ç¯å¢ƒæ•°æ®åº“
        
        Args:
            db_config: æ•°æ®åº“é…ç½®
            table_name: è¡¨å
            well_id: äº•ID
            parameter: å‚æ•°åç§°ï¼ˆå­—æ®µåï¼‰
            limit: æ•°æ®é‡é™åˆ¶ï¼ˆæœ€å¤§50000ï¼‰
            start_date: å¼€å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼‰
            date_field: ç”¨äºæ—¶é—´ç­›é€‰çš„å­—æ®µåï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰
        
        Returns:
            list: å‚æ•°åºåˆ—æ•°æ®ï¼Œæ¯æ¡è®°å½•åŒ…å« value, date_time_index ç­‰å­—æ®µ
        """
        try:
            # å¼ºåˆ¶é™åˆ¶ï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
            MAX_LIMIT = 50000
            # [ä¼˜åŒ–] å¤„ç† limit=None çš„æƒ…å†µ
            if limit is None:
                limit = MAX_LIMIT
            else:
                limit = min(int(limit), MAX_LIMIT)
            print(f"ğŸ”’ æŸ¥è¯¢äº•å‚æ•°åºåˆ—: well_id={well_id}, parameter={parameter}, limit={limit}")
            
            # è·å–æ•°æ®åº“è¿æ¥
            connection_string = DatabaseService.get_connection_string(db_config, 'utf8')
            engine = DatabaseService.create_engine(connection_string)
            
            # æ„å»ºè¡¨å
            schema = db_config.get('schema', 'public')
            quoted_table = DatabaseService.quote_identifier(table_name)
            if schema and schema != 'public':
                quoted_schema = DatabaseService.quote_identifier(schema)
                full_table = f"{quoted_schema}.{quoted_table}"
            else:
                full_table = quoted_table
            
            # æ„å»ºå­—æ®µå
            quoted_wid = DatabaseService.quote_identifier('wid')
            quoted_param = DatabaseService.quote_identifier(parameter)
            
            # ç¡®å®šæ—¶é—´å­—æ®µ
            if date_field:
                # ç”¨æˆ·æŒ‡å®šäº†æ—¶é—´å­—æ®µï¼Œç›´æ¥ä½¿ç”¨
                time_field = date_field
                print(f"âœ… ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ—¶é—´å­—æ®µ: {time_field}")
            else:
                # å°è¯•è‡ªåŠ¨æ‰¾æ—¶é—´å­—æ®µï¼ˆå¸¸è§çš„å­—æ®µåï¼‰
                time_field_candidates = ['date_time_index', 'datetime', 'timestamp', 'time', 'date', 'update_date']
                time_field = None
                
                # æŸ¥è¯¢è¡¨ç»“æ„æ‰¾æ—¶é—´å­—æ®µ
                with engine.connect() as conn:
                    # å…ˆæ£€æŸ¥è¡¨ä¸­æœ‰å“ªäº›å­—æ®µ
                    inspect_query = f"""
                        SELECT column_name 
                        FROM information_schema.columns 
                        WHERE table_name = '{table_name}'
                        AND table_schema = '{schema}'
                    """
                    df_columns = pd.read_sql(text(inspect_query), conn)
                    available_columns = df_columns['column_name'].tolist()
                    
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ—¶é—´å­—æ®µ
                    for candidate in time_field_candidates:
                        if candidate in available_columns:
                            time_field = candidate
                            break
                    
                    if not time_field:
                        # å¦‚æœæ‰¾ä¸åˆ°æ—¶é—´å­—æ®µï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªçœ‹èµ·æ¥åƒæ—¥æœŸçš„å­—æ®µ
                        for col in available_columns:
                            if any(keyword in col.lower() for keyword in ['date', 'time']):
                                time_field = col
                                break
                
                if not time_field:
                    time_field = 'date_time_index'  # é»˜è®¤å­—æ®µå
                    print(f"âš ï¸  æœªæ‰¾åˆ°æ˜ç¡®çš„æ—¶é—´å­—æ®µï¼Œä½¿ç”¨é»˜è®¤å€¼: {time_field}")
            
            quoted_time = DatabaseService.quote_identifier(time_field)
            
            # æ„å»ºWHEREå­å¥
            where_clauses = [f"{quoted_wid} = '{well_id}'"]
            
            if start_date:
                where_clauses.append(f"{quoted_time} >= '{start_date}'")
            if end_date:
                where_clauses.append(f"{quoted_time} <= '{end_date}'")
            
            where_clause = " AND ".join(where_clauses)
            
            # æ„å»ºæŸ¥è¯¢ï¼ˆè·å–æœ€æ–°æ•°æ®ï¼ŒæŒ‰æ—¶é—´å€’åºï¼‰
            query = f"""
                SELECT {quoted_param} as value, {quoted_time} as date_time_index
                FROM {full_table}
                WHERE {where_clause}
                ORDER BY {quoted_time} DESC
                LIMIT {limit}
            """
            
            print(f"æ‰§è¡ŒæŸ¥è¯¢: {query}")
            
            # æ‰§è¡ŒæŸ¥è¯¢
            with engine.connect() as conn:
                df = pd.read_sql(text(query), conn)
            
            # æŒ‰æ—¶é—´æ­£åºæ’åˆ—ï¼ˆæ¨¡å‹éœ€è¦ï¼‰
            df = df.sort_values('date_time_index')
            
            print(f"âœ… æˆåŠŸè·å– {len(df)} æ¡äº•å‚æ•°æ•°æ®")
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            return df.to_dict('records')
            
        except Exception as e:
            print(f"âŒ è·å–äº•å‚æ•°åºåˆ—å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            raise Exception(f"è·å–äº•å‚æ•°åºåˆ—å¤±è´¥: {str(e)}")
    
    # @staticmethod
    # def generate_anomaly_excel(anomalies, metadata):
    #     """ç”Ÿæˆå¼‚å¸¸æ£€æµ‹ExcelæŠ¥å‘Šï¼ˆé€šç”¨ï¼‰"""
    #     try:
    #         import io
    #         import pandas as pd
            
    #         if not anomalies:
    #             raise ValueError("æ²¡æœ‰å¼‚å¸¸æ•°æ®å¯å¯¼å‡º")
                
    #         # è½¬æ¢ä¸ºDataFrame
    #         df = pd.DataFrame(anomalies)
            
    #         # ç»Ÿä¸€åˆ—åæ˜ å°„
    #         column_mapping = {
    #             'row_index': 'è¡Œå·',       # [æ–°å¢] æ˜ å°„è¡Œå·
    #             'field_name': 'æ•°æ®åº“å­—æ®µ',
    #             # DrillingData å­—æ®µ
    #             'parameter': 'å‚æ•°åç§°',
    #             'rawValue': 'å¼‚å¸¸æ•°å€¼',
    #             'unit': 'å•ä½',
    #             'type': 'å¼‚å¸¸ç±»å‹',
    #             'timestamp': 'æ—¶é—´æˆ³',
    #             # ProductData å­—æ®µ
    #             'code': 'ç‚¹ä½ä»£ç ',
    #             'value': 'å¼‚å¸¸æ•°å€¼',
    #             'details': 'è¯¦ç»†æè¿°',
    #             'time_range': 'å½±å“æ—¶æ®µ'
    #         }
            
    #         # é‡å‘½å
    #         df = df.rename(columns=column_mapping)
            
    #         # [ä¿®æ”¹] è°ƒæ•´åˆ—é¡ºåºï¼Œå°†â€œè¡Œå·â€æ”¾åœ¨æœ€å‰é¢
    #         desired_order = [
    #             'è¡Œå·', 'æ•°æ®åº“å­—æ®µ', 'å‚æ•°åç§°', 'ç‚¹ä½ä»£ç ', 
    #             'å¼‚å¸¸æ•°å€¼', 'å•ä½', 'å¼‚å¸¸ç±»å‹', 
    #             'æ—¶é—´æˆ³', 'è¯¦ç»†æè¿°', 'å½±å“æ—¶æ®µ'
    #         ]
            
    #         # é‡æ–°æ’åˆ—åˆ—é¡ºåº
    #         existing_cols = [c for c in desired_order if c in df.columns]
    #         other_cols = [c for c in df.columns if c not in existing_cols]
    #         df = df[existing_cols + other_cols]
            
    #         # åˆ›å»º Excel
    #         output = io.BytesIO()
    #         with pd.ExcelWriter(output, engine='openpyxl') as writer:
    #             df.to_excel(writer, index=False, sheet_name='å¼‚å¸¸æ˜ç»†')
                
    #             if metadata:
    #                 meta_rows = []
    #                 for k, v in metadata.items():
    #                     meta_rows.append({'é…ç½®é¡¹': k, 'å†…å®¹': str(v)})
    #                 meta_df = pd.DataFrame(meta_rows)
    #                 meta_df.to_excel(writer, index=False, sheet_name='æ£€æµ‹ç¯å¢ƒé…ç½®')
    #                 worksheet = writer.sheets['æ£€æµ‹ç¯å¢ƒé…ç½®']
    #                 worksheet.column_dimensions['A'].width = 20
    #                 worksheet.column_dimensions['B'].width = 50
            
    #         output.seek(0)
    #         return output
            
    #     except Exception as e:
    #         raise Exception(f"ç”ŸæˆExcelæŠ¥å‘Šå¤±è´¥: {str(e)}")